from typing import Dict, Tuple
import torch
import torch.nn as nn
from omegaconf import OmegaConf
import torch.nn.functional as F

from diffusion_policy.model.common.normalizer import LinearNormalizer
from diffusion_policy.policy.base_lowdim_policy import BaseLowdimPolicy
from diffusion_policy.model.bet.action_ae.discretizers.k_means import KMeansDiscretizer
from diffusion_policy.model.bet.latent_generators.mingpt import MinGPT
from diffusion_policy.model.bet.utils import eval_mode
from diffusion_policy.model.common.slice import slice_episode

class BETLowdimPolicy(BaseLowdimPolicy):
    def __init__(self, 
            action_ae: KMeansDiscretizer, 
            obs_encoding_net: nn.Module, 
            state_prior: MinGPT,
            gamma,
            device,
            horizon,
            n_action_steps,
            n_obs_steps,
            beta=1.0):
        super().__init__()
    
        self.normalizer = LinearNormalizer()
        self.action_ae = action_ae
        self.obs_encoding_net = obs_encoding_net
        self.state_prior = state_prior
        self.horizon = horizon
        self.n_action_steps = n_action_steps
        self.n_obs_steps = n_obs_steps
        self.gamma = gamma
        self.beta = beta
        self.device = device

    # ========= inference  ============
    def predict_action(self, obs_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        obs_dict: must include "obs" key
        result: must include "action" key
        """
        assert 'obs' in obs_dict
        assert 'past_action' not in obs_dict # not implemented yet
        nobs = self.normalizer['obs'].normalize(obs_dict['obs'])
        B, _, Do = nobs.shape
        To = self.n_obs_steps
        T = self.horizon

        # pad To to T
        obs = torch.full((B,T,Do), -2, dtype=nobs.dtype, device=nobs.device)
        obs[:,:To,:] = nobs[:,:To,:]

        # (B,T,Do)
        enc_obs = self.obs_encoding_net(obs)

        # Sample latents from the prior
        latents, offsets = self.state_prior.generate_latents(enc_obs)

        # un-descritize
        naction_pred = self.action_ae.decode_actions(
            latent_action_batch=(latents, offsets)
        )
        # (B,T,Da)

        # un-normalize
        action_pred = self.normalizer['action'].unnormalize(naction_pred)

        # get action
        start = To - 1
        end = start + self.n_action_steps
        action = action_pred[:,start:end]
        result = {
            'action': action,
            'action_pred': action_pred
        }
        return result

    def action_prob(self, obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """
        Calculate the probability of the given action under the current policy.

        Args:
            obs_dict: A dictionary containing the observation, must include "obs" key.
            action: The action tensor generated by `predict_action`.

        Returns:
            A tensor representing the probability of the given action under the policy.
        """

        # Normalize the observation
        nobs = self.normalizer['obs'].normalize(obs)
        B, _, Do = nobs.shape
        To = self.n_obs_steps
        T = self.horizon

        # Pad observations to match the horizon length
        obs = torch.full((B, T, Do), -2, dtype=nobs.dtype, device=nobs.device)
        obs[:, :To, :] = nobs[:, :To, :]

        # Encode the observations
        enc_obs = self.obs_encoding_net(obs)

        # Encode the given action into latent representation
        latent = self.action_ae.encode_into_latent(action)

        # Get the logits and offsets (if applicable) from the state prior
        if self.action_ae.predict_offsets:
            logits, offsets = self.state_prior.generate_latents(enc_obs)
        else:
            logits = self.state_prior.generate_latents(enc_obs)

        # Calculate probabilities using softmax
        probs = F.softmax(logits, dim=-1)

        # Gather probabilities corresponding to the provided action
        if self.action_ae.predict_offsets:
            action_latent, _ = latent
        else:
            action_latent = latent

        action_latent = action_latent.view(-1)  # Flatten the action latent indices
        action_probs = probs.view(-1, probs.shape[-1])
        selected_probs = action_probs[torch.arange(action_latent.shape[0]), action_latent]

        # Reshape to match the batch and sequence dimensions
        selected_probs = selected_probs.view(B, -1)

        return selected_probs

    # ========= training  ============
    def set_normalizer(self, normalizer: LinearNormalizer):
        self.normalizer.load_state_dict(normalizer.state_dict())
    
    def fit_action_ae(self, input_actions: torch.Tensor):
        self.action_ae.fit_discretizer(input_actions=input_actions)
    
    def get_latents(self, latent_collection_loader):
        training_latents = list()
        with eval_mode(self.action_ae, self.obs_encoding_net, no_grad=True):
            for observations, action, mask in latent_collection_loader:
                obs, act = observations.to(self.device, non_blocking=True), action.to(self.device, non_blocking=True)
                enc_obs = self.obs_encoding_net(obs)
                latent = self.action_ae.encode_into_latent(act, enc_obs)
                reconstructed_action = self.action_ae.decode_actions(
                    latent,
                    enc_obs,
                )
                total_mse_loss += F.mse_loss(act, reconstructed_action, reduction="sum")
                if type(latent) == tuple:
                    # serialize into tensor; assumes last dim is latent dim
                    detached_latents = tuple(x.detach() for x in latent)
                    training_latents.append(torch.cat(detached_latents, dim=-1))
                else:
                    training_latents.append(latent.detach())
        training_latents_tensor = torch.cat(training_latents, dim=0)
        return training_latents_tensor

    def get_optimizer(
            self, weight_decay: float, learning_rate: float, betas: Tuple[float, float]
        ) -> torch.optim.Optimizer:
        return self.state_prior.get_optimizer(
                weight_decay=weight_decay, 
                learning_rate=learning_rate, 
                betas=tuple(betas))
    
    def compute_loss(self, batch):
        # normalize input
        assert 'valid_mask' not in batch
        observations_1 = batch["obs"].to(self.device)
        actions_1 = batch["action"].to(self.device)
        votes_1 = batch["votes"].to(self.device)
        observations_2 = batch["obs_2"].to(self.device)
        actions_2 = batch["action_2"].to(self.device)
        votes_2 = batch["votes_2"].to(self.device)

        threshold = 1e-2
        diff = torch.abs(votes_1 - votes_2)
        condition_1 = (votes_1 > votes_2) & (diff >= threshold)  # votes_1 > votes_2 and diff >= threshold
        condition_2 = (votes_1 < votes_2) & (diff >= threshold)  # votes_1 < votes_2 and diff >= threshold

        votes_1 = torch.where(condition_1, torch.tensor(1.0, device=self.device), torch.tensor(0.0, device=self.device))
        votes_1 = torch.squeeze(votes_1, dim=-1).detach()
        votes_2 = torch.where(condition_2, torch.tensor(1.0, device=self.device), torch.tensor(0.0, device=self.device))
        votes_2 = torch.squeeze(votes_2, dim=-1).detach()

        threshold = 1e-2
        diff = torch.abs(votes_1 - votes_2)
        condition_1 = (votes_1 > votes_2) & (diff >= threshold)  # votes_1 > votes_2 and diff >= threshold
        condition_2 = (votes_1 < votes_2) & (diff >= threshold)  # votes_1 < votes_2 and diff >= threshold

        votes_1 = torch.where(condition_1, torch.tensor(1.0, device=self.device), torch.tensor(0.0, device=self.device))
        votes_1 = torch.squeeze(votes_1, dim=-1).detach()
        votes_2 = torch.where(condition_2, torch.tensor(1.0, device=self.device), torch.tensor(0.0, device=self.device))
        votes_2 = torch.squeeze(votes_2, dim=-1).detach()

        batch_1 = {
            'obs': torch.tensor(observations_1, device=self.device),
            'action': torch.tensor(actions_1, device=self.device),
        }

        batch_2 = {
            'obs': torch.tensor(observations_2, device=self.device),
            'action': torch.tensor(actions_2, device=self.device),
        }

        nbatch_1 = self.normalizer.normalize(batch_1)
        nbatch_2 = self.normalizer.normalize(batch_2)

        obs_1 = nbatch_1['obs']
        action_1 = nbatch_1['action']
        obs_2 = nbatch_2['obs']
        action_2 = nbatch_2['action']

        obs_1 = slice_episode(obs_1, horizon=self.horizon, stride=self.horizon)
        action_1 = slice_episode(action_1, horizon=self.horizon, stride=self.horizon)
        obs_2 = slice_episode(obs_2, horizon=self.horizon, stride=self.horizon)
        action_2 = slice_episode(action_2, horizon=self.horizon, stride=self.horizon)

        traj_loss_1, traj_loss_2 = 0, 0

        for i in range(len(obs_1)):
            obs_1_slide = obs_1[i]
            obs_1_slide = obs_1_slide[:, :self.n_obs_steps, ...]
            action_1_slide = action_1[i]
            action_1_slide = action_1_slide[:, :self.n_action_steps, ...]
            prob_1 = self.action_prob(obs_1_slide, action_1_slide)
            traj_loss_1 += prob_1 * (self.gamma ** (i*self.horizon + torch.arange(0, self.horizon, device=self.device)))
            

            obs_2_slide = obs_2[i]
            obs_2_slide = obs_2_slide[:, :self.n_obs_steps, ...]
            action_2_slide = action_2[i]
            action_2_slide = action_2_slide[:, :self.n_action_steps, ...]
            prob_2 = self.action_prob(obs_2_slide, action_2_slide)
            traj_loss_2 += prob_2 * (self.gamma ** (i*self.horizon + torch.arange(0, self.horizon, device=self.device)))

        traj_loss_1 = torch.sum(traj_loss_1, dim=-1)
        traj_loss_2 = torch.sum(traj_loss_2, dim=-1)

        mle_loss_1 = -F.logsigmoid(traj_loss_1 - traj_loss_2)
        mle_loss_2 = -F.logsigmoid(traj_loss_2 - traj_loss_1)

        loss = (votes_1.to(self.device) * mle_loss_1 + votes_2.to(self.device) * mle_loss_2) / 2

        return torch.mean(loss)
