ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libglew.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
nohup: ignoring input
ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libglew.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libglew.so' from LD_PRELOAD cannot be preloaded (cannot open shared object file): ignored.
[2025-05-11 02:46:41,081][diffusion_policy.model.diffusion.conditional_unet1d][INFO] - number of parameters: 2.771304e+08
Error executing job with overrides: []
Traceback (most recent call last):
  File "/home/miaorunqing/project/Distributional-DPO-Robotics-master/train.py", line 33, in main
    workspace.run()
  File "/home/miaorunqing/project/Distributional-DPO-Robotics-master/diffusion_policy/workspace/pbrl_diffusion_realrobot_online_type1_workspace.py", line 82, in run
    ref_policy.to(device)
  File "/home/miaorunqing/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py", line 927, in to
    return self._apply(convert)
  File "/home/miaorunqing/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/miaorunqing/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  File "/home/miaorunqing/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py", line 579, in _apply
    module._apply(fn)
  [Previous line repeated 5 more times]
  File "/home/miaorunqing/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py", line 602, in _apply
    param_applied = fn(param)
  File "/home/miaorunqing/miniforge3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py", line 925, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 23.69 GiB total capacity; 399.31 MiB already allocated; 81.00 MiB free; 422.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
